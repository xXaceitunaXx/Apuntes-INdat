\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}

\colorlet{LightGray}{White!90!Periwinkle}
\colorlet{LightOrange}{Orange!15}
\colorlet{LightGreen}{Green!15}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\declaretheoremstyle[name=Theorem,]{thmsty}
\declaretheorem[style=thmsty,numberwithin=section]{theorem}
\tcolorboxenvironment{theorem}{colback=LightGray}

\declaretheoremstyle[name=Proposition,]{prosty}
\declaretheorem[style=prosty,numberlike=theorem]{proposition}
\tcolorboxenvironment{proposition}{colback=LightOrange}

\declaretheoremstyle[name=Principle,]{prcpsty}
\declaretheorem[style=prcpsty,numberlike=theorem]{principle}
\tcolorboxenvironment{principle}{colback=LightGreen}

\setstretch{1.2}
\geometry{
    textheight=9in,
    textwidth=5.5in,
    top=1in,
    headheight=12pt,
    headsep=25pt,
    footskip=30pt
}

% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------

\title{ \normalsize \textsc{}
\\ [2.0cm]
\HRule{1.5pt} \\
\LARGE \textbf{\uppercase{Template Title}
\HRule{2.0pt} \\ [0.6cm] \LARGE{Very Cool Subtitle} \vspace*{10\baselineskip}}
}

\author{\textbf{Author} \\
    Victor Elvira Fernández, Tomás Ruiz Rojo, Juan Horrillo Crespo \\
    Universidad de Valladolid \\
    \date{\today}}

\maketitle
\newpage

\tableofcontents
\newpage

% ------------------------------------------------------------------------------


\section{Propiedades asintóticas del EMV y del estadístico RV}


\subsection{Estimadores en muestras grandes}

Nuestro principal objetivo es analizar cómo se comporta la muestra cuando su tamaño es tan grande como nosotros queramos.
\setlength{\parskip}{1em}   % Aumentar espacio entre párrafos

Situación:
\setlength{\parskip}{0em}   % Aumentar espacio entre párrafos

\( X_1, X_2, \dots, X_n \) variables aleatorias independientes igualmente distribuidas (v.a.i.i.d.) tal que \( P_\theta:\theta  \in  \Theta\).

Tenemos nuestra muestra aleatoria simple (m.a.s) con \( n \) grande y nos interesa estimar \( \theta \) (o \( g(\theta) \)).

\setlength{\parskip}{1em}   % Aumentar espacio entre párrafos
Cuando el tamaño de la muestra aumenta, también aumenta la información disponible de \( \theta \) (o \( g(\theta) \)), por lo tanto se espera que la estimación sea más precisa.

Si tenemos un estimador \( T(X_1, \dots, X_n) \) razonable, cuando \( n \) aumenta, el estimador \( T(X_1, \dots, X_n) \) deberá ser más preciso.

\setlength{\parskip}{0em}   % Aumentar espacio entre párrafos
Lo que podemos esperar es que \( T(X_1, \dots, X_n) \) esté próximo a \( \theta \) con mayor probabilidad.

\subsubsection{Consistencia de un estimador}

Se dice que un estimador es consistente si cumple:
\setlength{\parskip}{1em}

\[\forall \epsilon > 0, \; \delta > 0 \quad \exists N: n \geq N\]
\[P_\theta\left(|T(X_1, \dots, X_n) - \theta| < \epsilon\right) \geq 1 - \delta\]

\textbf{\textit{Definición: }} \(T_n(X)\) es un estimador consistente para \(\theta\) (o \(g(\theta)\)) si:

\[
    \forall \epsilon > 0 \quad P_\theta\left(|T(X_1, \dots, X_n) - \theta| \leq \epsilon\right) \xrightarrow{n \to \infty} 1
\]

es decir, si:

\[
    T_n(x) \xrightarrow{\underset{n \to \infty}{\text{P}}} \theta \quad \text{o} \quad \lim_{n \to \infty} \forall \epsilon > 0 \quad P_\theta\left(|T(X_1, \dots, X_n) - \theta| \leq \epsilon\right) = 1
\]

\newpage
\subsubsection*{Estrategias para comprobar si un estimador es consistente}

\begin{enumerate}
    \setlength{\parskip}{1em}
    \item \textbf{Si converge en probabilidad.}
          Resultado:
          \[
              Si \quad T_n(x) \xrightarrow{\underset{n \to \infty}{\text{P}}} \theta \quad \text{y} \quad T_n(x) \xrightarrow{\underset{n \to \infty}{\text{P}}} \theta'
          \]
          \begin{itemize}
              \item \(T_n(x)+G_n(x) \xrightarrow{\underset{n \to \infty}{\text{P}}} \theta + \theta '\)
              \item \(T_n(x)G_n(x) \xrightarrow{\underset{n \to \infty}{\text{P}}} \theta\theta '\)
              \item Si \(\theta' \neq 0, \frac{T_n(x)}{G_n(x)} \xrightarrow{\underset{n \to \infty}{\text{P}}} \frac{\theta}{\theta '}\)
          \end{itemize}

    \item \textbf{Utilizando la ley débil de los grandes números.}
          \(X_1, X_2, \dots, X_n\) i.i.d. con \(E(X_i)=\mu < \infty\)
          \setlength{\parskip}{0em}
          \[\frac{1}{n}\sum_{i=1}^{n} X_i \xrightarrow{\underset{n \to \infty}{\text{P}}} \mu\]

          \setlength{\parskip}{1em}
          Los momentos muestrales son estimaciones consistentes de los correspondientes momentos poblacionales

    \item \textbf{Utilizando la convergencia en media cuadrática.}
          Un estimador $T(X_1, \dots, X_n)\xrightarrow{\underset{n \to \infty}{\text{m.c.}}} \theta$ si:
          \begin{itemize}
              \item \(E(T(X_1, \dots, X_n)) \xrightarrow{{n \to \infty}} \theta\), sesgo(\(T(X_1, \dots, X_n)\)) \(\xrightarrow{{n \to \infty}} 0\)
              \item \(\text{Var}(T(X_1, \dots, X_n)) \xrightarrow{{n \to \infty}} 0\)
          \end{itemize}
          Es decir que si ECM ($T(X_1, \dots, X_n)$) \(\xrightarrow{{n \to \infty}} 0\), el estimador converge en media cuadrática. Un estimador insesgado sería consistente si su varianza converge a 0 con \(n \to \infty\)

          El resultado es que la convergencia en media cuadrática es más fuerte que la convergencia en probabilidad.
\end{enumerate}
\noindent\rule{\textwidth}{0.2pt} % Línea de separación

\subsubsection*{Ejercicio 1}
\(X_1, \dots, X_n \) v.a.i.i.d \quad E(X_i)=\mu, \text{Var}(X_i)=\sigma^2 \quad \forall i=1, \dots, n

¿Es consistente $T_n(x) = \frac{X_1 + X_2 + \dots + X_n}{\frac{n}{2}}$?

Usando la ley de los grandes números:
\(\frac{1}{n}\sum X_i \xrightarrow{\text{P}} \mu \quad T_n(x)\xrightarrow{\text{P}}2\mu\)

Resultado: El estimador \(T_n(x)\) NO es consistente

\newpage

La consistencia por si sola no es tan interesante, ya que si $T_n \text{es consistente para } \theta$, nos dice que para n grande los errores serán pequeños pero no nos permite conocer el orden del error $\left(\frac{1}{n}, \frac{1}{\sqrt{n}},\frac{1}{\log(n)}, \dots\right)$

Si $\{K_n\}_{n=1}^{\infty}$ es una sucesión de reales positivos y $\epsilon>0$ definimos $P_n(\epsilon)$.

\(P_n(\epsilon)=P_\theta(|T_n(x)-\theta| \leq \frac{\epsilon}{K_n})\)

Habiendo definido \(P_n(\epsilon)\), ¿qué pasará con  \(P_n(\epsilon)\) cuando n sea grande?
\begin{enumerate}
    \item Si \(K_n\) crece "lentamente" (por ejemplo:  \(K_n=\log(n)\)), el error disminuye "lentamente" a medida que aumenta n \(P_n(\epsilon) \xrightarrow{{\text{n} \to \infty}} 1\). Si \(K_n\) crece lentamente, \( \frac{\epsilon}{K_n}\) es más grande, lo que facilitará que el error esté por debajo del umbral.
    \item Si \(K_n\) crece "rápido" (por ejemplo:  \(K_n=n\)), el error disminuye más rápido a medida que aumenta n \(P_n(\epsilon) \xrightarrow{{\text{n} \to \infty}} 0\).  Si \(K_n\) crece rápido, \( \frac{\epsilon}{K_n}\) se hace muy pequeño y se hace muy difícil que el error sea tan pequeño.
    \item Casos intermedios.

          Si \(K_n\) crece "adecuadamente", \(P_n(\epsilon) \xrightarrow{{\text{n} \to \infty}} H(\epsilon) \in (0,1)\). Decimos que el error converge a 0 a velocidad \(\frac{1}{K_n}\)
\end{enumerate}
De manera resumida:

\(
P_n(\epsilon) = P_\theta(K_n|T_n(x) - \theta| \leq \epsilon) \xrightarrow{{n \to \infty}

\left\{
\begin{array}{l}
    0 \text{ si } K_n \xrightarrow{\infty} \text{ "rápido"}                                  \\[1em]
    0 \leq P_n(\epsilon) \leq 1 \text{ si } K_n \xrightarrow{\infty} \text{ "adecuadamente"} \\[1em]
    1 \text{ si } K_n \xrightarrow{\infty} \text{ "lento"}
\end{array}
\right\}
\)

La idea es que al multiplicar $K_n$, se amplifica la velocidad de convergencia de los errores a 0. Si elegimos $K_n$ adecuadamente, de forma que $P_n(\epsilon)$ sea menor que 1, podemos controlar la velocidad a la que los errores tienden a 0, mejorando la precisión.

\newpage

% ------------------------------------------------------------------------------
% Reference and Cited Works
% ------------------------------------------------------------------------------

\begin{thebibliography}{9}
    \bibitem{diapos1}
    ¿Quién?, \\ \textit{¿Qué?}. \\ ¿Dónde?.
\end{thebibliography}

% ------------------------------------------------------------------------------

\end{document}
