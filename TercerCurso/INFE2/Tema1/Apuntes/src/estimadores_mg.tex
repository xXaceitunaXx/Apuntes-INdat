\section{Estimadores en muestras grandes}

Sean $X_1, X_2, \dots, X_n$ variables aleatorias independientes igualmente distribuidas (v.a.i.i.d.) tal que $P_\theta:\theta \in \Theta$. Tenemos nuestra muestra aleatoria simple (m.a.s) con $n$ grande y nos interesa estimar $\theta$ (o $g(\theta)$).

Cuando el tamaño de la muestra aumenta, también aumenta la información disponible de $\theta$ (o $g(\theta)$), por lo tanto se espera que la estimación sea más precisa. Esto podemos comprobarlo al analizar la distribución de la media muestral ya que como $\overline{X}\thicksim N\left(\mu, \frac{\sigma^2}{n}\right)$, entonces $Var(\overline{X}) \xrightarrow{n \rightarrow \infty} 0$.

\begin{figure}[h!]
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[no markers, domain=0:10, samples=100, axis lines*=left, xlabel=Test,
                    ylabel=, height=6cm, width=10cm, enlargelimits=false, clip=false, axis on top,
                    grid = major,legend style={at={(0.5,-0.1)},anchor=north, legend columns=-1}]

                \addplot [fill=cyan!20, draw=blue, domain=-4:4] {gauss(0,0.6)} \closedcycle;
                \addlegendentry{N(0,0.8)}
                \addplot [fill=orange!20, fill opacity=0.5, draw=red, domain=-4:4] {gauss(0,1)} \closedcycle;
                \addlegendentry{N(0,1)}
            \end{axis}
        \end{tikzpicture}
        \caption{Comparación aumento de n}
    \end{center}
\end{figure}

Consideraremos que un estimador $T(X_1, \dots, X_n)$ es razonable si cuando $n$ aumenta, el estimador $T(X_1, \dots, X_n)$ es más preciso. Lo que podemos esperar es que $T(X_1, \dots, X_n)$ esté próximo a $\theta$ con mayor probabilidad. \\

A continuación, se definirán los criterios que nos permitirán comprobar la calidad de información que nos proporciona un estimador.

\subsection{Consistencia de un estimador}

Se dice que un estimador $T(X_1,\dots,X_n)$ es consistente si cumple que

\[
    \forall \varepsilon > 0, \delta > 0 \quad \exists N: n \geq N
\]
\[
    P_\theta(|T(X_1, \dots, X_n) - \theta| < \varepsilon) \geq 1 - \delta
\]

\textbf{\textit{Definición: }} $T_n(X)$ es un estimador consistente para $\theta$ (o $g(\theta)$) si

\[
    \forall \varepsilon > 0 \quad P_\theta\left(|T(X_1, \dots, X_n) - \theta| \leq \varepsilon\right) \xrightarrow{n \to \infty} 1
\]

es decir, si:

\[
    T_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta \quad \text{o} \quad \lim_{n \to \infty} \forall \varepsilon > 0 \quad P_\theta(|T(X_1, \dots, X_n) - \theta| \leq \varepsilon) = 1
\]

\newpage

Existen algunas estrategias para comprobar si un estimador es consistente:
\begin{enumerate}
    \item \textbf{Convergencia en probabilidad}. Si $T_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta$ y $G_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta'$
          \begin{itemize}
              \item $T_n(x)+G_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta + \theta '$
              \item $T_n(x) \cdot G_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta \cdot \theta '$
              \item Si $\theta' \neq 0, \frac{T_n(x)}{G_n(x)} \overset{p}{\underset{n \to \infty}{\to}} \frac{\theta}{\theta '}$
          \end{itemize}

    \item \textbf{Ley débil de los grandes números}. $X_1, X_2, \dots, X_n$ i.i.d. con $E(X_i)=\mu < \infty$
          \[\frac{1}{n}\sum_{i=1}^{n} X_i \overset{p}{\underset{n \to \infty}{\to}} \mu\]
          Los momentos muestrales son estimaciones consistentes de los correspondientes momentos poblacionales.

    \item \textbf{Convergencia en media cuadrática}. Un estimador $T(X_1, \dots, X_n)\xrightarrow{\underset{n \to \infty}{\text{m.c.}}} \theta$ si:
          \begin{itemize}
              \item $E(T(X_1, \dots, X_n)) \xrightarrow{{n \to \infty}} \theta$, sesgo de $T(X_1, \dots, X_n)\xrightarrow{{n \to \infty}} 0$
              \item $Var(T(X_1, \dots, X_n)) \xrightarrow{{n \to \infty}} 0$
          \end{itemize}
          Es decir que si ECM $T(X_1, \dots, X_n) \xrightarrow{{n \to \infty}} 0$, el estimador converge en media cuadrática. Un estimador insesgado sería consistente si su varianza converge a 0 con $n \to \infty$. La convergencia en media cuadrática es más fuerte que la convergencia en probabilidad.
\end{enumerate}

\begin{exercise}
    Sean $X_1, \dots, X_n$ v.a.i.i.d. con $E(X_i)=\mu$ y $Var(X_i)=\sigma^2$, $\forall i=1, \dots, n$. ¿Es consistente $T_n(x) = \frac{X_1 + X_2 + \dotsb + X_n}{\frac{n}{2}}$?

    Usando la ley de los grandes números:
    \[
        \frac{1}{n}\sum X_i \xrightarrow{\text{P}} \mu \quad T_n(x)\xrightarrow{P}2\mu
    \]

    Resultado: El estimador $T_n(x)$ NO es consistente
\end{exercise}

La consistencia por si sola no es tan interesante, ya que si $T_n$ es consistente para $\theta$, nos dice que para n grande los errores serán pequeños pero no nos permite conocer el orden del error $\left(\frac{1}{n}, \frac{1}{\sqrt{n}},\frac{1}{\log(n)}, \dots\right)$

Si $\{K_n\}_{n=1}^{\infty}$ es una sucesión de reales positivos y $\varepsilon>0$ definimos $P_n(\varepsilon)$ como

\[P_n(\varepsilon)=P_\theta\left(|T_n(x)-\theta| \leq \frac{\varepsilon}{K_n}\right)\]

Habiendo definido $P_n(\varepsilon)$, ¿qué pasará con $P_n(\varepsilon)$ cuando $n$ sea grande?
\begin{enumerate}
    \item Si $K_n$ crece "lentamente" (por ejemplo:  $K_n=\log(n)$), el error disminuye "lentamente" a medida que aumenta n $P_n(\varepsilon) \xrightarrow{{\text{n} \to \infty}} 1$. Si $K_n$ crece lentamente, $ \frac{\varepsilon}{K_n}$ es más grande, lo que facilitará que el error esté por debajo del umbral.
    \item Si $K_n$ crece "rápido" (por ejemplo:  $K_n=n$), el error disminuye más rápido a medida que aumenta n $P_n(\varepsilon) \xrightarrow{{\text{n} \to \infty}} 0$.  Si $K_n$ crece rápido, $ \frac{\varepsilon}{K_n}$ se hace muy pequeño y se hace muy difícil que el error sea tan pequeño.
    \item Casos intermedios. Si $K_n$ crece "adecuadamente", $P_n(\varepsilon) \xrightarrow{{\text{n} \to \infty}} H(\varepsilon) \in (0,1)$. Decimos que el error converge a 0 a velocidad $\frac{1}{K_n}$
\end{enumerate}

De manera resumida:

\[
    P_n(\varepsilon) = P_\theta(K_n|T_n(x) - \theta| \leq \varepsilon) \xrightarrow{n \to \infty}\begin{Bmatrix}
        0 \text{ si } K_n \xrightarrow{\infty} \text{rápido}                                  \\
        0 \leq P_n(\varepsilon) \leq 1 \text{ si } K_n \xrightarrow{\infty} \text{adecuadamente} \\
        1 \text{ si } K_n \xrightarrow{\infty} \text{lento}
    \end{Bmatrix}
\]

La idea es que al multiplicar $K_n$, se amplifica la velocidad de convergencia de los errores a 0. Si elegimos $K_n$ adecuadamente, de forma que $P_n(\varepsilon)$ sea menor que 1, podemos controlar la velocidad a la que los errores tienden a 0, mejorando la precisión.

\subsection{Estimador Consistente Asintoticamente Normal (CAN)}

\textbf{\textit{Definición: }} Sean $X_1,\dots,X_n$ v.a.i.i.d. con distribución $P_\theta$ donde $\theta \in \Theta$, entonces un estimador $T_n(X)=T(X_1,\dots,X_n)$ con $n\geq 1$ será CAN para $\theta$ si

\[
    \exists \sigma_n(\theta) > 0, \theta \xrightarrow{n\to\infty}0 \quad \text{tal que}\quad \frac{T_n(X)-\theta}{\sigma_n(\theta)}\overset{L}{\to}N(0,1)
\]

En muchas ocasiones, $\sigma^2_n(\theta)$ es de la forma $\frac{Var_\theta(T_n(X))}{n}$ y se llama varianza asintótica del estimador. En este caso $T_n(X)$ es CAN para $\theta$ si

\[
    \sqrt{n}(|T_n(X)-\theta|) \overset{L}{\to} N(0, Var_\theta(T_n(X)))
\]

donde los errores convergen a 0 con velocidad $\frac{1}{\sqrt{n}}$.

Al igual que la LGN es clave para obtener estimadores consistentes, el TCL lo es para obtener estimadores CAN. \\
Sean $X_1,\dots,X_n$ v.a.i.i.d. tal que $E(X_i)=\mu$ y $Var(X_i)=\sigma^2 < \infty$. Entonces

\begin{itemize}
    \item $\frac{\sum{X_i} - E(\sum{X_i})}{\sqrt{Var(\sum{X_i})}} \overset{L}{\to} N(0,1)$
    \item $\sqrt{n}\frac{\overline{X}-\mu}{\sigma}\overset{L}{\to}N(0,1)$
    \item $\overline{X}\thicksim N\left(\mu,\frac{\sigma^2}{n}\right)$
\end{itemize}

Resulta útil aplicar el $\Delta$-método cuando queremos aproximar la distribución de una función no lineal de un estimador que es asintoticamente normal.\\
Sean $X_1,\dots,X_n$ v.a.i.i.d. donde su distribución es $P_\theta$ tal que $\theta \in \Theta \subseteq \mathbb{R}$ y sea uan función $g:\Theta \to \mathbb{R}$ con derivada no nula. Si $T_n(X)$ es CAN para $\theta$ tal que $\sqrt{n}(|T_n(X)-\theta|)\overset{L}{\to}N(0,V^2_t(\theta))$ entonces

\[
    \sqrt{n}(|g(T_n(X))-g(\theta)|)\overset{L}{\to}N(0,V^2_t(\theta)g'(\theta)^2)
\]


\subsection*{Ejercicio 1}

Sean $X_1, \dots, X_n$ v.a.i.i.d. donde $E(X_i)=\mu,\quad Var(X_i)=\sigma^2 \quad \forall i=1 \dots n$. Determinar si los siguientes estimadores son consistentes
\begin{enumerate}[label=\alph*)]\setcounter{enumi}{1}
    \item $T(X_1,\dots,X_n)=\frac{T(X_1 + \dots + X_{\frac{n}{2}})}{\frac{n}{2}}$\\
          Vemos que
          \[
              T(X_1,\dots,X_{\frac{n}{2}}) = \frac{\sum_{i=1}^{\frac{n}{2}}}{\frac{n}{2}} = \bar{X}_{\frac{n}{2}} \xrightarrow{P} \mu
          \]
          Por tanto el estimador es consistente

    \item $T(X_1,\dots,X_n)=X_1$\\
          El estimador no es consistente ya que $X_1$ no depende de n. Lo mínimo para que pueda converger es que dependa de n.

    \item $T(X_1,\dots,X_n)=2  \sum_{i=1}^{n}\frac{i  X_i}{n(n+1)}$\\
          No podemos usar la ley de los grandes números porque $X_1, 2X_2, 3X_3, \dots$ no son v.a.i.i.d. por tanto usaremos la convergencia en media cuadrática.

          \[
              T_n(x) \xrightarrow{m.c.} \mu
              \left\{
              \begin{array}{l}
                  E(T_n(X)) \xrightarrow[n \to \infty]{} \mu \\
                  Var(T_n(X)) \xrightarrow[n \to \infty]{} 0
              \end{array}
              \right.
          \]
          \[
              E(T_n(x))=\frac{2}{n  (n+1)}  E((\sum_{i=1}^{n} i)  X_i)=\frac{2}{n  (n+1)}  (\sum_{i=1}^{n} i)   E(X_i)=
          \]
          \[
              =\frac{2  \mu}{n  (n+1)}  \sum_{i=1}^{n} i=\frac{2  \mu}{n  (n+1)} \frac{n  (n+1)}{2}= \mu
          \]
          Se cumple que la media tiende a mu cuando n tiende a infinito.

          \[
              Var(T_n(x))=\frac{4}{n^2  (n+1)^2}  \sum_{i=1}^{n} Var(i X_i)= \frac{4}{n^2  (n+1)^2}  \sigma^2  \sum_{i=1}^{n} i^2
          \]
          \[
              =\frac{4}{n^2  (n+1)^2}  \sigma^2  \frac{n  (n+1)  (2n+1)}{6}=\frac{2}{3} \frac{2n+1}{n^2+n} \sigma^2 \xrightarrow[n \to \infty]{} 0
          \]
          Como se cumple que la media tiende a 0 cuando n tiende a infinito el estimador es consistente.
\end{enumerate}

\subsection*{Ejercicio 3}
Sean $X_1, \dots, X_n$ v.a.i.i.d. con distribución uniforme $U(0,\theta)$, ¿$2 \bar{X}$ es consistente para $\theta$?
\[
    E(X_i)=\frac{a+b}{2}=\frac{\theta}{2};\quad Var(X_i)=\frac{(b-a)^2}{12}
\]
Entonces
\[
    \bar{X} \xrightarrow[n \to \infty]{P} \implies 2 \bar{X} \xrightarrow{P} \theta
\]

Por tanto $2 \bar{X}$ es consistente para $\theta$

\newpage

\subsection{Información de Fisher}

La información de Fisher $I_x(\theta)$ es la matriz que mide la cantidad
de información que una m.a.s. contiene sobre el estimador.

\textbf{\textit{Definición: }} Sea $X=(X_1 \dots X_n)$ v.a.i.i.d. con distribución $P_\theta \in P=\{P_\theta : \theta \in \Theta \subseteq \mathbb{R}\}$ con función de densidad $f(X,\theta)$ y en la que existe $\frac{d f(x,\theta)}{d \theta}$, la información de Fisher sobre $\theta$ contenida en X es:
\[
    I_x(\theta) = Var_\theta (S(\theta,X))
\]
\[
    Score=S_x(\theta)=S(\theta,X)=\frac{d}{\mathrm{d\theta}}\log f(x,\theta)
\]
\subsection{Condiciones de regularidad de Cramer-Rao (CRCR)}

Llamaremos familias regulares a aquellas familias en las que se verifican las
condiciones de regularidad de Cramer-Rao.
\\ Estas son las familias con las que trabajaremos.
\\ \\\textbf{Condiciones de regularidad de Cramer-Rao:}
\begin{enumerate}
    \item El espacio paramétrico es un intervalo de $\mathbb{R}$.
    \item El soporte de la distribución no depende del parámetro $\theta$.
          Por ejemplo $x=\{x:f(x,\theta) > 0 \}$, no depende de $\theta$ y sería regular. En cambio U(0,$\theta$)
          tiene un soporte que depende de $\theta$, por lo que no es regular
    \item Se pueden calcular las dos primeras derivadas bajo el signo integral.
          Además se puede intercambiar la derivada con el signo integral.
          \[
              \frac{d}{d \theta} \int_{x} f(x,\theta)  \,dx = \int_{x} \frac{d}{d \theta} f(x,\theta)  \,dx
          \]
    \item $T_n(x)$ es un estimador insesgado para $\theta$ o $g(\theta)$.
\end{enumerate}

Bajo las condiciones de regularidad de Cramer-Rao, podemos definir la cantidad
de información esperada como:
\[
    I_x(\theta)=E_\theta(S(\theta,X)^2)=E_\theta\left(\left(\frac{d}{d \theta} \log f(x,\theta)\right)^2\right)
\]

\begin{proofs}
    Deberemos probar que $E_\theta(\left(\frac{d}{d \theta} \log f(x,\theta)\right))=0$. Si lo conseguimos entonces, $Var(S(\theta,x))=E(S(\theta,x)^2)-E(S(\theta,x))^2=E(S(\theta,x)^2)$
    \[
        E_\theta\left(\left(\frac{d}{d \theta} \log f(x,\theta)\right)\right)=\int_{x} E_\theta\left(\left(\frac{d}{d \theta} \log f(x,\theta)\right)\right)  f(x,\theta) \,dx =
    \]
    \[
        =\int_{x} \frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)}  f(x,\theta) \,dx=\int_{x} \frac{d}{d \theta} f(x,\theta) \,dx = 0
    \]
\end{proofs}

Si se verifican las condiciones de regularidad de Cramer-Rao, otra forma alternativa de calcular la información de Fisher es:
\[
    I_x(\theta)=-E\left(\frac{d^2}{d \theta^2} \log f(x,\theta)\right)
\]

\begin{proofs}
    Breve paso previo:
    \[
        \frac{d}{d \theta}\left(\frac{d}{d \theta} \log f(x,\theta)\right)=\frac{d}{d \theta}\left(\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)}\right)=
    \]
    donde derivando el cociente
    \[
        =\frac{\frac{d^2}{d \theta} f(x,\theta) f(x,\theta) -\left(\frac{d}{d \theta} f(x,\theta)\right)^2}{f(x,\theta)^2}=\frac{\frac{d^2}{d \theta} f(x,\theta)}{f(x,\theta)} -\left(\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)}\right)^2
    \]
    Demostración:
    \[
        E(\frac{d^2}{d \theta} \log f(x,\theta))=\int_{x} \frac{d}{d \theta}\left(\frac{d}{d \theta} \log f(x,\theta)\right)  f(x,\theta) \,dx =
    \]
    \[
        =\int_{x} \frac{d^2}{d \theta} f(x, \theta) \,dx - \int_{x}\left(\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)}\right)^2  f(x,\theta) \,dx
    \]
    \[
        = -\int_{x}\left(\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)}\right)^2  f(x,\theta) \,dx=-I_x(\theta)
    \]
\end{proofs}

\textbf{\textit{Propiedades:}}

Sean X e Y dos variables independientes de la misma familia de distribuciones

$X \sim P_\theta, \quad \theta \in \Theta \subseteq \mathbb{R}, \quad f(x,\theta), I_x(\theta)$

$Y \sim Q_\theta, \quad \theta \in \Theta \subseteq \mathbb{R}, \quad g(y,\theta), I_y(\theta)$

Entonces
\begin{enumerate}
    \item Propiedad de la información de Fisher conjunta: $I_{xy}(\theta) = I_x(\theta)+I_y(\theta)$
          \begin{proofs} Por independencia
              \[
                  f_{xy}(x,y,\theta)=f_x(x,\theta)  f_y(y,\theta)
              \]
              \[
                  I_{xy}(\theta)=Var\left(\frac{d}{d \theta} \log (f_x(x,\theta)  f_y(y,\theta))\right)=Var\left(\frac{d}{d \theta} (\log f_x(x,\theta) + \log f_y(y,\theta))\right)
              \]
              Y por las propiedades de la varianza: $Var(X+Y)= Var(X)+Var(Y)$ si $X$ e $Y$ son independientes
              \[
                  =Var\left(\frac{d}{d \theta} (\log f_x(x,\theta))\right) + Var\left(\frac{d}{d \theta} (\log f_y(y,\theta))\right)=I_x(\theta)+I_y(\theta)
              \]
          \end{proofs}
    \item Sean $X_1, \dots, X_n$ m.a.s. v.a.i.i.d. $P_\theta,\theta \in \Theta$ con $f(x,\theta)$ de una familia regular:
          \[
              I_{X_1,\dots,X_n}=n  I_{X_1}(\theta)
          \]
\end{enumerate}

\subsection*{Ejemplo con la distribucion de Bernoulli:}

Sea $X \sim B(p) \to f(x,p)=p^x (1-p)^{1-x}$ donde $x=\{0,1\}$. \\Usando la primera definición de $I_x(p)$:
\[
    I_x(p)=Var((S_x(p)))=Var\left(\frac{d}{dp} \log f(x,p)\right)
\]
donde
\[
    log f(x,p)=x \log p + (1-x)\log(1-p) \implies S_x(p)=\frac{d}{dp}(x \log (p)+ (1-x)\log (1-p))
\]
por tanto
\[
    Var(S_x(p))=Var\left(\frac{x-p}{p(1-p)}\right)=\frac{Var(x)}{p^2(1-p)^2}=\frac{p(1-p)}{p^2(1-p)^2}=\frac{1}{p(1-p)}
\]

\subsection*{Ejemplo con la distribucion de Poisson:}

Sea $X \sim Poisson(\lambda) \to f(k,\lambda)=\frac{\lambda^ke^{-\lambda}}{k!}$ donde $k \in \mathbb{N}_0$. \\Usando la primera definición de $I_x(p)$:
\[
    I_x(\lambda)=Var(S_x(\lambda)); \quad S_x(\lambda) = \frac{d}{d \lambda} \log\left(\frac{\lambda^x e^{-\lambda}}{x!}\right)=\frac{x-\lambda}{\lambda}
\]
\[
    I_x(\lambda) = Var\left(\frac{x-\lambda}{\lambda}\right)=\frac{1}{\lambda^2}Var(x-\lambda)=\frac{\lambda}{\lambda^2}=\frac{1}{\lambda}
\]

\subsection*{Ejemplo con n muestras de la distribucion de Bernoulli:}

Sean $X_1,\dots,X_n$ v.a.i.i.d. donde $X_i \sim B(p)$ entonces la densidad conjunta será
\[
    f(X_1,\dots,X_n)=\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i}=p^{\sum_{i=1}^{n} x_i}(1-p)^{n-\sum_{i=1}^{n} x_i}
\]
como
\[
    \log f(x_1,...,x_n)=(\sum_{i=1}^{n}x_i) \log p + (n-\sum_{i=1}^{n}x_i) \log (1-p)
\]
entonces
\[
    S_{x_1,...,x_n}(p)=\frac{d}{dp} \log f(X_1,...,X_n)=\frac{\sum_{i=1}^{n}x_i - np}{p(1-p)}
\]
y finalmente
\[
    Var\left(\frac{\sum_{i=1}^{n}x_i - np}{p(1-p)}\right)=\frac{\sum_{i=1}^{n}(Var (x_i))}{p^2(1-p)^2}=\frac{n}{p(1-p)}I_{x_1,...,x_n}(p)=n  I_{x_1}(p)=\frac{n}{p(1-p)}
\]

\newpage

\subsection*{Ejemplo con la exponencial:}

Sea $X \sim exp(\lambda) \to f(x,\lambda)=\lambda e^{-\lambda x}$
simplemente como
\[
    \log f(x,\theta)=\log \lambda - \lambda X \implies S_x(\lambda)=\frac{1}{\lambda}-X
\]
entonces
\[
    I_x (\lambda)=-E_\lambda\left(\frac{d}{d \lambda} S_x(\lambda)\right)=-E_\lambda\left(\frac{-1}{\lambda^2}\right)=\frac{1}{\lambda^2}
\]

\noindent Vamos a profundizar un poco en las condiciones de regularidad de Cramer-Rao:

Sea X=$(X_1,\dots,X_n)$ m.a.s de $P_\theta,\theta \in \Theta \subseteq \mathbb{R}$
,con densidad f(x,$\theta)$ y con $T_n(x)$ como estimador insesgado de g($\theta$).

\[
    E(T_n(x)=g(\theta)) \to g(\theta)=\int_x T_n(x) f(x,\theta) \,dx
\]

$g(\theta)$ es diferenciable respecto a $\theta$ bajo el signo integral y podemos intercambiar derivada e integral

\subsection{Cota de Cramer-Rao}
Sea $X_,\dots,X_n$ m.a.s $P_\theta,\theta \in \Theta \subseteq \mathbb{R}$ con densidad
$f(x,\theta)$ y sea $T_n(x)$ un estimador insesgado de $g(\theta)$ con Var$(T_n(x)) < \infty$
y que verifica las condiciones de regularidad de Cramer-Rao, entonces:
\[
    Var(T_n(x)) \geq \frac{(g'(\theta))^2}{n \cdot I_{x_1}(\theta)}
\]

\begin{proofs}
    Consideremos $Cov(T_n(x),S_x(\theta))=E(T_n(x)\cdot S_x(\theta))$

\[
Cov(T_n(x)S_x(\theta))=\int_x T_n(x) \cdot S_x(\theta) \cdot f(x,\theta) \,dx
\]\[
\int_x T_n(x) \cdot \frac{d}{d \theta} \log f(x,\theta) \cdot f(x,\theta) \,dx
=\int_x T_n(x) \cdot \frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)} \cdot f(x,\theta) \,dx
\]\[
\int_x T_n(x) \cdot \frac{d}{d \theta} f(x,\theta)\,dx
= \frac{d}{d \theta} \int_x T_n(x) f(x,\theta) \,dx
=\frac{d}{d \theta} E(T_n(x))
\]
Como $T_n(x)$ es insesgado:
\[
=\frac{d}{d \theta} g(\theta)=g'(\theta)
\]

Usamos la desigualdad de Cauchy-Schwarz que relaciona varianza y covarianza
\[
cov(X,Y)=\sqrt{Var(X) \cdot Var(Y)}
\]
entonces,
\[
(g'(\theta)^2)=(cov(T_n(x)\cdot S_x(\theta)))^2 \leq Var(T_n(x)) \cdot Var(S_x(\theta))
=Var(T_n(x)) \cdot n \cdot I_{x_1}(\theta)
\]
\[
    Var(T_n(x)) \geq \frac{(g'(\theta))^2}{n \cdot I_{x_1}(\theta)}
\]

\end{proofs}

La varianza de un estimador insesgado es como mínimo $\frac{(g'(\theta))^2}{n \cdot I_{x_1}(\theta)}$

Nota: Bajo las mismas condiciones de regularidad de Cramer-Rao puede estimarse
en el caso que $T_n(x)$ sea un estimador de $\theta$ con $Var(T_n(x))<\infty$

\[
    Var(T_n(x)) \geq \frac{1}{n \cdot I_{x_1}(\theta)}= \frac{1}{I_n(\theta)}
\]


La demostración es anaáloga a la anterior.

\subsubsection*{Ejemplo}


\(
X_1,\dots,X_n \sim B(p) \quad p \in (0,1)
\\I_n(\theta)=n \cdot I_{x_1}(\theta)=\frac{n}{p(1-p)}
\)

Si quiero estimar p, ¿cuál es la cota CR para cualquier estimador insesgado de P?

\(
Var(T_n(x)) \geq \frac{1}{n \cdot I_{x_1}(p)}=\frac{p(1-p)}{n}
\)

Supongamos que nuestro estimador para p es $\bar{X}$

$Var(\frac{\sum_{i=1}^{n}}{n})=\frac{1}{n^2}Var(\sum_{i=1}^{n} x_i)
    =\frac{n \cdot p(1-p)}{n^2}=\frac{p(1-p)}{n}$

Hemos visto que,como consecuencia del Teorema Central del Limite,  la velocidad de convergencia de los errores a cero es del orden $\frac{1}{\sqrt{n}}$

Si $X_1,\dots,X_n$ i.i.d. $P_\theta \in P,\theta \in \Theta \subseteq \mathbb{R}$ y f($x,\theta$)
verifica las condiciones de regularidad de Cramer-Rao, es decir, si la familia es regular.

$\sqrt{n}(T_n(x)-g(\theta)) \xrightarrow{L} N(0,V_T^2(\theta))$

donde $V_T^2(\theta)$ es la varianza asintótica del estimador y $\sqrt{n}$ es la normalización
adecuada como consecuencia del TCL.

Es equivalente, $T_n(x) \simeq N(g(\theta),\frac{V_T^2(\theta)}{n})$

Bajo esas condiciones de regularidad, la varianza $\frac{V_T^2(\theta)}{n}$
cumple también la cota de Cramer-Rao

\subsubsection*{Ejemplo:}

\(
X_1,\dots,X_n \quad i.i.d \quad U(0,\theta) \quad \theta>0
\\f(x,\theta)= \frac{1}{\theta} \quad
\)
(Ejercicio 3 apartado b)

Se demuestra que $X_{(n)}$ es un estimador consistente para $\theta$, pero la velocidad de convergencia no es $\frac{1}{\sqrt{n}}$

\subsection{Estimador Consistente Asintóticamente Normal (CAN) y \texorpdfstring{\\}{ } Asintóticamente Eficiente (AE)}

Sea $X_1,\dots,X_n$ m.a.s. de $P_\theta, \theta \in \Theta \subseteq \mathbb{R}$
con densidad $f(x,\theta)$ y $T_n(x)$ estimador insestado de g($\theta$) con una Var($T_n(x))<\infty$
y que verifica las condiciones de regularidad de Cramer-Rao, entonces:

\[
    Var(T_n(x)) \geq \frac{(g'(\theta))^2}{n \cdot I_{x_1}(\theta)}
\]

es decir,

\[
    \frac{V_T^2(\theta)}{n} \geq \frac{(g'(\theta))^2}{n \cdot I_{x_1}(\theta)}
\implies     V_T^2(\theta) \geq \frac{(g'(\theta))^2}{I_{x_1}(\theta)}
\]

Concretamente un estimador CAN será mejor cuanto menor sea la varianza asintótica.
Si la varianza del estimador original es igual a la cota de Cramer-Rao, decimos que $T_n(x)$ es CAN y asintóticamente eficiente (AE)

\textbf{\textit{Definición: }}$X_1,\dots,X_n$ m.a.s.$P_\theta, \theta \in \Theta \subseteq \mathbb{R}, f(x,\theta), T_n(x)$
es un estimador CAN y AE de g($\theta$) si es CAN y $V_T^2=\frac{(g'(\theta))^2}{I_{x_1}(\theta)}$, es decir si
\[
    \sqrt{n}(T_n(x)-g(\theta))\xrightarrow{L}N\left(0,\frac{(g'(\theta))^2}{I_{x_1}(\theta)}\right)
\]


\subsubsection*{Ejemplo:}

\(
X \sim P(\lambda) \quad \lambda>0 \quad f(x,\theta)=\frac{e^{-\lambda}\cdot \lambda^x}{x!} \quad x=0,1,2,\dots
\\ E(X)=\lambda \quad Var(X)=\lambda
\\ \log F(x,\theta)=-\lambda + x \cdot \log \lambda - \log x!
\)

Buscamos un estimador CAN y AE para $\lambda$.

Por el teorema central del límite, la media muestral es CAN para la media poblacional.

\[
\sqrt{n}(\bar{X}-\lambda) \xrightarrow{L} N(0,\lambda)
\quad o \quad \bar{X} \simeq N(\lambda,\frac{\lambda}{n})
\]

Para comprobar si $\bar{X}$ es AE, debemos comprobar que $\frac{\lambda}{n}$ es igual a $\frac{1}{n \cdot I_{x_1}(\lambda)}$

\[
S_x(\lambda)=\frac{x-\lambda}{\lambda}, \quad I_{x_1}=\frac{1}{\lambda}
\]

La varianza de $\bar{X}$ coincide con la cota de Cramer-Rao, por lo que es un estimador
$\\$ asintóticamente eficiente
$\left(\frac{1}{n\cdot I_{x_1}(\lambda)}=\frac{1}{n\cdot \frac{1}{\lambda}}=\frac{\lambda}{n}\right)$

\subsubsection*{Ejemplo:}

\(
X \sim exp(\lambda) \quad f(x,\lambda)=\lambda \cdot e^{-\lambda \cdot x} \quad x>0, \quad \lambda>0
\\ E(X) = \frac{1}{\lambda} \quad Var(X)=\frac{1}{\lambda^2}
\\ \log f(x,\lambda)= \log \lambda - \lambda \cdot X
\\ S(\lambda,x)=\frac{1}{\lambda}-X
\\ \sqrt{n}(\bar{X}-\frac{1}{\lambda}) \xrightarrow{L} N(0,\frac{1}{\lambda^2})
\\ \bar{X} \simeq N(\frac{1}{\lambda},\frac{1}{n \cdot \lambda^2})
\)

Sabemos que $\bar{X}$ es CAN para $\frac{1}{\lambda}$, es decir para
$g(\lambda)=\frac{1}{\lambda} ;  g'(\lambda)=\frac{-1}{\lambda^2}$
.

Queremos ver si tambien es AE para la media poblacional. Tenemos que comprobar que $\frac{V_T^2(\lambda)}{n}=\frac{1}{\lambda^2}$
coincida con la cota CR.

\[
I_{x_1}=\frac{1}{\lambda^2} \quad \frac{(g'(\theta))^2}{n \cdot I_{x_1}(\theta)}=
\frac{(\frac{-1}{\lambda^2})^2}{n \cdot \frac{1}{\lambda^2}}=\frac{1}{n \cdot \lambda^2}
\]

¿Y como obtendríamos un estimador AE para $\lambda$?
Utilizando el delta método.

Si $T_n(x)$ es CAN para $\theta \in \Theta \subseteq \mathbb{R}$ y g es una función con derivada no nula,
entonces $g(T_n(x))$ es CAN para $g(\theta)$ con varianza $\frac{V_T^2}{n}\cdot (g'(\theta))^2$

$$\sqrt{n}(T_n(x)-\theta) \xrightarrow{L}N(0,V_T^2(\theta))$$

$$\sqrt{n}(g(T_n(x))-g(\theta)) \xrightarrow{L} N(0,V_T^2(\theta)\cdot(g'(\theta))^2)$$

Volviendo al ejemplo, tras esta breve parte teórica, ya vimos que $\sqrt{n}(\bar{X}-\frac{1}{\lambda})\xrightarrow{L} N(0,\frac{1}{\lambda})$
Consideraremos que $\theta=\frac{1}{\lambda^2}$.

Pasos:
\begin{enumerate}
    \item $\sqrt{n}(\bar{X}-\theta)\xrightarrow{L}N(0,\theta^2)$
    \item Delta método:
          \[
          g(\theta)=\frac{1}{\theta} \quad;\quad g'(\theta)\frac{-1}{\theta^2}
          \]
          \[ 
          \sqrt{n}(g(\bar{X})-g(\theta)) \xrightarrow{L} N(0,\theta^2(\frac{-1}{\theta^2})^2)
          \]
          \[
          \sqrt{n}(\frac{1}{\bar{X}}-\frac{1}{\theta}) \xrightarrow{L} N(0,\frac{1}{\theta^2})
          \]
    \item $ \sqrt{n}(\frac{1}{\bar{X}}-\lambda) \xrightarrow{L} N(0,\lambda^2) \quad o \quad
              \frac{1}{\bar{X}}\eqsim N(\lambda,\frac{\lambda^2}{n})$
\end{enumerate}

Entonces, $\frac{1}{\bar{X}}$ es CAN para $\lambda$ y $\frac{V_T^2(\lambda)}{n}=\frac{\lambda^2}{n}$

Ahora nos cuestionamos, ¿es $\frac{1}{\lambda}$ AE?

Teniamos que $I_{x_1}(\lambda)=\frac{1}{\lambda^2}$, por lo que la cota CR será $\frac{1}{n \cdot I_{x_1}(\lambda)}=\frac{\lambda^2}{n}$.
Es un estimador asintóticamente eficiente.

\textbf{Resultado: }Todos los estimadores CAN que se obtienen usando el delta método a partir de un estimador asintóticamente eficiente,
son asintóticamente eficientes.


\begin{proofs}
    Sea $T_n(X)$ estimador CAN para $\theta$
    \[
        \sqrt{n}(T_n(X)-\theta) \overset{L}{\to}N(0,V^2_T(\theta))
    \]

    y $T_n(X)$ es AE entonces

    \[
        V^2_T(\theta) = \frac{1}{I_T(\theta)} \implies I_T(\theta)=\frac{1}{V^2_T(\theta)}
    \]

    Sea $g$ una función con derivada no nula, se tiene

    \[
        \sqrt{n}(g(T_n(X))-g(\theta)) \overset{L}{\to}N(0,V^2_T(\theta)(g'(\theta))^2)
    \]

    La varianza de $g(T_n(X))$ es

    \[
        Var(g(T_n(X)))=\frac{V^2_T(\theta)(g'(\theta))^2}{n}
    \]

    Y su cota CR es

    \[
        \frac{(g'(\theta))^2}{nI_{X_1}(\theta)}=\frac{(g'(\theta))^2}{\frac{n}{V^2_T(\theta)}}=\frac{V^2_T(\theta)(g'(\theta))^2}{n}\square
    \]
\end{proofs}
\newpage
\subsubsection{Estimadores razonables}

Los estimadores CAN y AE son estimadores con propiedades razonables, pero podemos tener más de un estimador razonable.

Sean $T_n(x)$ estimador CAN para g($\theta$) y $G_n(x)$ estimador CAN para g($\theta$)
\begin{itemize}
    \item Si uno de los 2 es AE y el otro no, el estimador AE es mejor.
    \item Si ambos son AE, será mejor el que tenga una menor varianza asintótica($V_T^2$).
\end{itemize}

Para ver cual tiene una menor varianza asintótica, usamos la eficiencia relativa asintótica.

\textbf{\textit{Definición: }} $X_1,\dots,X_n$ i.i.d. $\{ P_\theta: \theta \in \Theta \}$, sean $T_n(x)$
y $G_n(x)$ dos estimadores CAN de $g(\theta)$

\[
    ARE_{TG}(\theta)=\frac{V_G^2(\theta)}{V_T^2(\theta)}
\]

Si $ARE_{TG}(\theta)$ es menor que 1, $G_n(x)$ es mejor. En cambio si ARE es mayor que 1, $T_n(x)$ es mejor.

\newpage
