
\setlength{\parskip}{1em}
\subsubsection*{Ejercicio 1}

$X_1, \dots, X_n$ v.a.i.i.d. $\quad E(X_i)=\mu,\quad Var(X_i)=\sigma^2 \quad \forall i=1 \dots n$

b) $T(X_1,\dots,X_n)=\frac{T(X_1 + \dots + X_{\frac{n}{2}})}{\frac{n}{2}}$\\
$T(X_1,\dots,X_n) \xrightarrow{P} \mu \quad$ El estimador es consistente

c) $T(X_1,\dots,X_n)=X_1$\\
El estimador no es consistente ya que $X_1$ no depende de n. Lo mínimo para que
pueda converger es que dependa de n.

d) $T(X_1,\dots,X_n)=2 \cdot \sum_{i=1}^{n}\frac{i \cdot X_i}{n \cdot (n+1)}$\\
No podemos usar la ley de los grandes números porque $X_1, 2 \cdot X_2, 3 \cdot X_3$ no son v.a.i.i.d. Usaremos la convergencia en media cuadrática.

\[
T_n(x) \xrightarrow{m.c.} \mu 
\left\{
\begin{array}{l}
    E(T_n(X)) \xrightarrow[n \to \infty]{} \mu \\
    Var(T_n(X)) \xrightarrow[n \to \infty]{} 0
\end{array}
\right.
\]
\(
E(T_n(x))=\frac{2}{n \cdot (n+1)} \cdot E((\sum_{i=1}^{n} i) \cdot X_i)
=\frac{2}{n \cdot (n+1)} \cdot (\sum_{i=1}^{n} i) \cdot  E(X_i)=
\frac{2 \cdot \mu}{n \cdot (n+1)} \cdot \sum_{i=1}^{n} i \\
=\frac{2 \cdot \mu}{n \cdot (n+1)} \frac{n \cdot (n+1)}{2}= \mu
\)

Se cumple que la media tiende a mu cuando n tiende a infinito.

\(
Var(T_n(x))=\frac{4}{n^2 \cdot (n+1)^2} \cdot \sum_{i=1}^{n} Var(i\cdot X_i)
= \frac{4}{n^2 \cdot (n+1)^2} \cdot \sigma^2 \cdot \sum_{i=1}^{n} i^2 \\
= \frac{4}{n^2 \cdot (n+1)^2} \cdot \sigma^2 \cdot \frac{n \cdot (n+1) \cdot (2n+1)}{6}
=\frac{2}{3}\cdot \frac{2n+1}{n^2+n} \sigma^2 \xrightarrow[n \to \infty]{} 0
\)

Se cumple que la media tiende a 0 cuando n tiende a infinito.

El estimador es consistente.

\subsubsection*{Ejercicio 3}
Sean $X_1, \dots, X_n$ v.a.i.i.d. con distribución uniforme $U(0,\theta)$, ¿$2 \bar{X}$ es consistente para $\theta$?
\[
E(X_i)=\frac{a+b}{2}=\frac{\theta}{2};\quad Var(X_i)=\frac{(b-a)^2}{12}
\]
Entonces 
\[
    \bar{X} \xrightarrow[n \to \infty]{P} \implies 2 \bar{X} \xrightarrow{P} \theta
\]

Por tanto $2 \bar{X}$ es consistente para $\theta$

\newpage

\subsection{Información de Fisher}

La información de Fisher $I_x(\theta)$ es la matriz que mide la cantidad
de información que una m.a.s. contiene sobre el estimador.

\textbf{\textit{Definición: }} Sea $X=(X_1 \dots X_n)$ v.a.i.i.d. con distribución $P_\theta \in P=\{P_\theta : \theta \in \Theta \subseteq \mathbb{R}\}$ con función de densidad $f(X,\theta)$ y en la que existe $\frac{d f(x,\theta)}{d \theta}$, la información de Fisher sobre $\theta$ contenida en X es:
\[
    I_x(\theta) = Var_\theta (S(\theta,X))
\]
\[
    Score=S_x(\theta)=S(\theta,X)=\frac{d}{\mathrm{d\theta}}\log f(x,\theta)
\]
\subsection{Condiciones de regularidad de Craner-Rao (CRCR)}

Llamaremos familias regulares a aquellas familias en las que se verifican las 
condiciones de regularidad de Craner-Rao.
\\ Estas son las familias con las que trabajaremos.
\\ \\\textbf{Condiciones de regularidad de Craner-Rao:}
\begin{enumerate}
    \item El espacio paramétrico es un intervalo de $\mathbb{R}$.
    \item El soporte de la distribución no depende del parámetro $\theta$.
    Por ejemplo $x=\{x:f(x,\theta) > 0 \}$, no depende de $\theta$ y sería regular. En cambio U(0,$\theta$) 
    tiene un soporte que depende de $\theta$, por lo que no es regular
    \item Se pueden calcular las dos primeras derivadas bajo el signo integral.
     Además se puede intercambiar la derivada con el signo integral.
    \[
        \frac{d}{d \theta} \int_{x} f(x,\theta)  \,dx = \int_{x} \frac{d}{d \theta} f(x,\theta)  \,dx
    \]
    \item $T_n(x)$ es un estimador insesgado para $\theta$ o $g(\theta)$.
\end{enumerate}

Bajo las condiciones de regularidad de Craner-Rao, podemos definir la cantidad
de información esperada como:
\[
    I_x(\theta)=E_\theta(S(\theta,X)^2)=E_\theta\left(\left(\frac{d}{d \theta} \log f(x,\theta)\right)^2\right)
\]

\newpage

\begin{proofs}
    Deberemos probar que $E_\theta(\left(\frac{d}{d \theta} \log f(x,\theta)\right))=0$.
    Si lo conseguimos probar entonces, $Var(S(\theta,x))=E(S(\theta,x)^2)
    -E(S(\theta,x))^2=E(S(\theta,x)^2)$

    \[
        E_\theta\left(\left(\frac{d}{d \theta} \log f(x,\theta)\right)\right)=
        \int_{x} E_\theta\left(\left(\frac{d}{d \theta} \log f(x,\theta)\right)\right) \cdot f(x,\theta) \,dx =
    \]
    \[
        =\int_{x} \frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)} \cdot f(x,\theta) \,dx 
        =\int_{x} \frac{d}{d \theta} f(x,\theta) \,dx = 0
    \]
\end{proofs}


Si se verifican las condiciones de regularidad de Crner-Rao, otra forma alternativa de calcular
la información de Fisher es:
\[
    I_x(\theta)=-E\left(\frac{d^2}{d \theta} \log f(x,\theta)\right)
\]

\begin{proofs}
    Breve paso previo:
    \[
        \frac{d}{d \theta}\left(\frac{d}{d \theta} \log f(x,\theta)\right)=\frac{d}{d \theta}\left(\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)}\right)=
    \]
    donde derivando el cociente
    \[
        =\frac{\frac{d^2}{d \theta} f(x,\theta) f(x,\theta) -\left(\frac{d}{d \theta} f(x,\theta)\right)^2}{f(x,\theta)^2}=\frac{\frac{d^2}{d \theta} f(x,\theta)}{f(x,\theta)} -\left(\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)}\right)^2
    \]
    Demostración:
    \[
        E(\frac{d^2}{d \theta} \log f(x,\theta))=
        \int_{x} \frac{d}{d \theta}\left(\frac{d}{d \theta} \log f(x,\theta)\right) \cdot f(x,\theta) \,dx =
    \]
    \[
        =\int_{x} \frac{d^2}{d \theta} f(x, \theta) \,dx - \int_{x}\left(\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)}\right)^2 \cdot f(x,\theta) \,dx  
    \]
    \[
        = -\int_{x}(\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)})^2 \cdot f(x,\theta) \,dx=-I_x(\theta)
    \]
\end{proofs}

\textbf{\textit{Propiedades:}}

Sean X e Y dos variables independientes de la misma familia de distribuciones.

$
X \sim P_\theta, \quad \theta \in \Theta \subseteq \mathbb{R}, \quad f(x,\theta), I_x(\theta)
$

$
Y \sim Q_\theta, \quad \theta \in \Theta \subseteq \mathbb{R}, \quad g(y,\theta), I_y(\theta)
$

\begin{enumerate}
    \item Propiedad de la información de Fisher conjunta: $I_{xy}(\theta) = I_x(\theta)+I_y(\theta)$
    
    Demostración:

    \(
    f_{xy}(x,y,\theta)=f_x(x,\theta) \cdot f_y(y,\theta)
    \\ I_{xy}(\theta)=Var(\frac{d}{d \theta} \log (f_x(x,\theta) \cdot f_y(y,\theta)))
    =Var(\frac{d}{d \theta} (\log f_x(x,\theta) + \log f_y(y,\theta)))
    \\ \text{(Por las propiedades de la varianza: Var(X+Y)= Var(X)+Var(Y) si X e Y son independientes)} 
    \\ =Var(\frac{d}{d \theta} (\log f_x(x,\theta))) + Var(\frac{d}{d \theta} (\log f_y(y,\theta)))
    =I_x(\theta)+I_y(\theta)    
    \)
    \item Sean $X_1, \dots, X_n$ m.a.s. v.a.i.i.d. $P_\theta,\theta \in \Theta$ con $f(x,\theta)$ de una familia regular:
    \[
    I_{X_1,\dots,X_n}=n \cdot I_{X_1}(\theta)
    \]
\end{enumerate}

\subsubsection*{Ejemplo con la distribucion de Bernoulli:}

\(
X \sim B(p) \to f(x,p)=p^x (1-p)^{1-x}  \quad x=0,1
\\ \\ \text{Usando la primera definición de }I_x(p):
\\ \\ I_x(p)=Var((S_x(p)))=Var(\frac{d}{dp} \log f(x,p))
\\ log f(x,p)=x \log p + (1-x)\log(1-p)
\\ S_x(p)=\frac{d}{dp}(x \log (p)+ (1-x)\log (1-p))
\\ Var(S_x(p))=Var(\frac{x-p}{p(1-p)})=\frac{Var(x)}{p^2(1-p)^2}=
\frac{p(1-p)}{p^2(1-p)^2}=\frac{1}{p(1-p)}
\)

\subsubsection*{Ejemplo con la distribucion de Poisson:}

\(
I_x(\lambda)=Var(S_x(\lambda))
\\ f(x,\lambda) = \frac{\lambda^x \cdot e^{-\lambda}}{x!}
\\ S_x(\lambda) = \frac{d}{d \lambda} \log\left(\frac{\lambda^x \cdot e^{-\lambda}}{x!}\right) = \frac{x-\lambda}{\lambda}
\\ I_x(\lambda) = Var(\frac{x-\lambda}{\lambda})=\frac{1}{\lambda^2}Var(x-\lambda)
=\frac{\lambda}{\lambda^2}=\frac{1}{\lambda}
\)

\subsubsection*{Ejemplo con n muestras de la distribucion de Bernoulli:}

\(
X_1,\dots,X_n \quad v.a.i.i.d. \quad B(p)
\\ \\ f(X_1,\dots,X_n)=\prod_{i=1}^{n} p^{x_i}(1-p)^{1-x_i}=p^{\sum_{i=1}^{n} x_i}
(1-p)^{n-\sum_{i=1}^{n} x_i}
\\ \log f(x_1,...,x_n)=(\sum_{i=1}^{n}x_i) \log p + (n-\sum_{i=1}^{n}x_i) \log (1-p)
\\ S_{x_1,...,x_n}(p)=\frac{d}{dp} \log f(X_1,...,X_n)=\frac{\sum_{i=1}^{n}x_i - np}{p(1-p)}
\\ Var(\frac{\sum_{i=1}^{n}x_i - np}{p(1-p)})=\frac{\sum_{i=1}^{n}(Var x_i)}{p^2(1-p)^2}=\frac{n}{p(1-p)}
I_{x_1,...,x_n}(p)=n \cdot I_{x_1}(p)=\frac{n}{p(1-p)}
\)

\subsubsection*{Ejemplo con la exponencial:}

\(
X \sim exp(\lambda)
\\ \\ f(x,\theta)=\lambda \cdot e^{-\lambda x}
\\ \log f(x,\theta)=\log \lambda - \lambda X
\\S_x(\lambda)=\frac{1}{\lambda}-X
\\ I_x (\lambda)=-E_\lambda(\frac{d}{d \lambda} S_x(\lambda))=-E_\lambda(\frac{-1}{\lambda^2})
=\frac{1}{\lambda^2}
\)

