
\setlength{\parskip}{1em}
\subsubsection*{Ejercicio 1}

$X_1, \dots, X_n$ v.a.i.i.d. $\quad E(X_i)=\mu,\quad Var(X_i)=\sigma^2 \quad \forall i=1 \dots n$

b) $T(X_1,\dots,X_n)=\frac{T(X_1 + \dots + X_{\frac{n}{2}})}{\frac{n}{2}}$\\
$T(X_1,\dots,X_n) \xrightarrow{P} \mu \quad$ El estimador es consistente

c) $T(X_1,\dots,X_n)=X_1$\\
El estimador no es consistente ya que $X_1$ no depende de n. Lo mínimo para que
pueda converger es que dependa de n.

d) $T(X_1,\dots,X_n)=2 \cdot \sum_{i=1}^{n}\frac{i \cdot X_i}{n \cdot (n+1)}$\\
No podemos usar la ley de los grandes números porque $X_1, 2 \cdot X_2, 3 \cdot X_3$ no son v.a.i.i.d. Usaremos la convergencia en media cuadrática.

\[
T_n(x) \xrightarrow{m.c.} \mu 
\left\{
\begin{array}{l}
    E(T_n(X)) \xrightarrow[n \to \infty]{} \mu \\
    Var(T_n(X)) \xrightarrow[n \to \infty]{} 0
\end{array}
\right.
\]
\(
E(T_n(x))=\frac{2}{n \cdot (n+1)} \cdot E((\sum_{i=1}^{n} i) \cdot X_i)
=\frac{2}{n \cdot (n+1)} \cdot (\sum_{i=1}^{n} i) \cdot  E(X_i)=
\frac{2 \cdot \mu}{n \cdot (n+1)} \cdot \sum_{i=1}^{n} i \\
=\frac{2 \cdot \mu}{n \cdot (n+1)} \frac{n \cdot (n+1)}{2}= \mu
\)

Se cumple que la media tiende a mu cuando n tiende a infinito.

\(
Var(T_n(x))=\frac{4}{n^2 \cdot (n+1)^2} \cdot \sum_{i=1}^{n} Var(i\cdot X_i)
= \frac{4}{n^2 \cdot (n+1)^2} \cdot \sigma^2 \cdot \sum_{i=1}^{n} i^2 \\
= \frac{4}{n^2 \cdot (n+1)^2} \cdot \sigma^2 \cdot \frac{n \cdot (n+1) \cdot (2n+1)}{6}
=\frac{2}{3}\cdot \frac{2n+1}{n^2+n} \sigma^2 \xrightarrow[n \to \infty]{} 0
\)

Se cumple que la media tiende a 0 cuando n tiende a infinito.

El estimador es consistente.

\subsubsection*{Ejercicio 3}

$X_1, \dots, X_n$ v.a.i.i.d. con distribución uniforme U($0,\theta$)

¿$2 \bar{X}$ es consistente para $\theta$?

\(
E(X_i)=\frac{a+b}{2}=\frac{\theta}{2};\quad Var(X_i)=\frac{(b-a)^2}{12}
\)

$\bar{X} \xrightarrow[n \to \infty]{P} \implies 2 \bar{X} \xrightarrow{P} \theta
$

$2 \bar{X}$ es consistente para $\theta$
\newpage

\subsubsection{Información de Fisher}

La información de Fisher $I_x(\theta)$ es la matriz que mide la cantidad
de información que una m.a.s. contiene sobre el estimador.

\textbf{\textit{Definición: }} Sea X=($X_1 \dots X_n$) v.a.i.i.d. con distribución
$P_\theta \in P=\{P_\theta : \theta \in \Theta \subseteq \mathbb{R}\}$
con función de densidad f(X,$\theta$) y en la que existe $\frac{d f(x,\theta)}{d \theta}$
, la información de Fisher sobre $\theta$ contenida en X es:
\[
I_x(\theta) = Var_\theta (S(\theta,X))
\]
\[
Score=S_x(\theta)=S(\theta,X)=\frac{d}{\mathrm{d\theta}}
\log f(x,\theta)
\]
\subsubsection{Condiciones de regularidad de Craner-Rao (CRCR)}

Llamaremos familias regulares a aquellas familias en las que se verifican las 
condiciones de regularidad de Craner-Rao.
\\ Estas son las familias con las que trabajaremos.

\textbf{Condiciones de regularidad de Craner-Rao:}
\begin{enumerate}
    \item El espacio paramétrico es un intervalo de $\mathbb{R}$.
    \item El soporte de la distribución no depende del parámetro $\theta$.
    
    Por ejemplo $x=\{x:f(x,\theta) > 0 \}$, no depende de $\theta$ y sería regular. En cambio U(0,$\theta$) 
    tiene un soporte que depende de $\theta$, por lo que no es regular
    \item Se pueden calcular las dos primeras derivadas bajo el signo integral.
     Además se puede intercambiar la derivada con el signo integral.
     
    \(
    \frac{d}{d \theta} \int_{x} f(x,\theta)  \,dx =
    \int_{x} \frac{d}{d \theta} f(x,\theta)  \,dx
    \)
    \item $T_n(x)$ es un estimador insesgado para $\theta$ o $g(\theta)$.
\end{enumerate}

Bajo las condiciones de regularidad de Craner-Rao, podemos definir la cantidad
de información esperada como:
\[
I_x(\theta)=E_\theta(S(\theta,X)^2)=E_\theta((\frac{d}{d \theta} \log f(x,\theta))^2)
\]
\textbf{\textit{Demostración:}}
Deberemos probar que $E_\theta((\frac{d}{d \theta} \log f(x,\theta)))=0$.
Si lo conseguimos probar entonces, $Var(S(\theta,x))=E(S(\theta,x)^2)
-E(S(\theta,x))^2=E(S(\theta,x)^2)$

\(
    E_\theta((\frac{d}{d \theta} \log f(x,\theta)))=
    \int_{x} E_\theta((\frac{d}{d \theta} \log f(x,\theta))) \cdot f(x,\theta) \,dx 
    =\int_{x} \frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)} \cdot f(x,\theta) \,dx 
    \\=\int_{x} \frac{d}{d \theta} f(x,\theta) \,dx = 0
\)

Si se verifican las condiciones de regularidad de Crner-Rao, otra forma alternativa de calcular
la información de Fisher es:
\[
I_x(\theta)=-E(\frac{d^2}{d \theta} \log f(x,\theta))
\]

\textbf{\textit{Demostración:}}

Breve paso previo:


\(
\frac{d}{d \theta} (\frac{d}{d \theta} \log f(x,\theta))=
\frac{d}{d \theta} (\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)})
\text{= (derivando el cociente)}
\frac{\frac{d^2}{d \theta} f(x,\theta) f(x,\theta) - (\frac{d}{d \theta} f(x,\theta))^2}{f(x,\theta)^2}
\\ = \frac{\frac{d^2}{d \theta} f(x,\theta)}{f(x,\theta)} - (\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)})^2
\)

Demostración:

\(
E(\frac{d^2}{d \theta} \log f(x,\theta))=
\int_{x} frac{d}{d \theta} (\frac{d}{d \theta} \log f(x,\theta)) \cdot f(x,\theta) \,dx = \\
\int_{x} \frac{d^2}{d \theta} f(x, \theta) \,dx - \int_{x} (\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)})^2 \cdot f(x,\theta) \,dx  
= - \int_{x} (\frac{\frac{d}{d \theta} f(x,\theta)}{f(x,\theta)})^2 \cdot f(x,\theta) \,dx  
= - I_x(\theta)
\)

\textbf{\textit{Propiedades:}}

Sean X e Y dos variables independientes de la misma familia de distribuciones.

$
X \sim P_\theta, \quad \theta \in \Theta \subseteq \mathbb{R}, \quad f(x,\theta), I_x(\theta)
$
$
Y \sim Q_\theta, \quad \theta \in \Theta \subseteq \mathbb{R}, \quad g(y,\theta), I_y(\theta)
$
