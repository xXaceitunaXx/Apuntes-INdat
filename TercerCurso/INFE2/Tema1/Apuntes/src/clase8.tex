\setlength{\parskip}{1em}
% Creo que el caso multiparamétrico como seccion empieza en la clase 7.
% Aun asi lo voy a poner como si empezara una subsección

\subsection{Caso multiparamétrico}

Situacion:

\(
X_1,\dots,X_n \quad i.i.d. \quad P_\theta,\theta \in \Theta \subseteq \mathbb{R}^s 
\\ \text{El parámetro s-dimensional es } \theta = (\theta_1,\dots,\theta_s) \quad s \geq 1
\\ \text{ con familia de densidad }\{ f(x,\theta),\theta_0 \in \Theta\}
\)

Igualmente podemos escribir

\begin{itemize}
    \item Función de verosimilitud: $L(\theta,X_1,\dots,X_n)=\prod^{n}_{i=1} f(x_i,\theta)$
    \item Función de log-verosimilitud: $l(\theta,X_1,\dots,X_n)=\sum^{n}_{i=1} \log f(x_i,\theta)$
    \item Vector/función score: $S(\theta,X_1,\dots,X_n)=S(X,\theta)=\frac{d}{d \theta} \log L(\theta,X)
    = (\frac{d}{d \theta} \log L(\theta_1,X),\dots,\frac{d}{d \theta} \log L(\theta,X))$
\end{itemize}

Ahora nos preguntamos, ¿cual es la cantidad de información de Fisher que tendremos en el caso multiparamétrico?.
Para averiguarlo recurrrimos a la matriz Hessiana, que recordamos que es la matriz s x s de las derivadas parciales de segundo orden

\(
I_{ij}(\theta)=E_\theta(-\frac{d^2}{d \theta_i d \theta_j} log L(\theta,X))
=E(\frac{d}{d \theta_i} log L(\theta,X),\frac{d}{d \theta_j} log L(\theta,X))
\)

\(
H(\theta,X)=\{ \frac{d^2}{d \theta_i d \theta_j}log L(\theta,X) \}^s 
=
\begin{pmatrix}
    \frac{d^2}{d \theta_1^2}log L(\theta,X) & \dots & \frac{d^2}{d \theta_1 d \theta_s}log L(\theta,X) \\
    \vdots & \ddots & \vdots \\
    \frac{d^2}{d \theta_s d \theta_1}log L(\theta,X) & \dots & \frac{d^2}{d \theta_s^2}log L(\theta,X)
\end{pmatrix}
\)

Sabiendo esto, $I(\theta)$,matriz de información esperada(que recordemos que es la matriz de covarianzas del vectir score) será:
\[
I(\theta) = E_\theta(-H(\theta,X))
\]
Además de esto tendremos tambíen la matriz de información de Fisher observada

\subsubsection{EMV en el caso multiparamétrico}

$\hat{\theta}=\hat(\theta_n)=(\hat(\theta_1),\dots,\hat(\theta_s))'$ es el EMV para $\theta=(\theta_1,\dots,\theta_s)$
si es la solución al siguiente sistema de ecuaciones:

\[
\begin{matrix}
    \frac{d}{d \theta_1} log L(\theta,X)=0 \\
    \dots \\
    \frac{d}{d \theta_s} log L(\theta,X)=0
\end{matrix}
\]

Estas ecuaciones son las denominadas \textbf{ecuaciones de verosimilitud}.

\subsubsection*{Propiedades asintóticas del EMV multiparamétrico}

En la situación $X_1,\dots,X_n$ i.i.d. $P_\theta, \theta \in \Theta \subseteq \mathbb{R}^s s\geq1$
y bajo las siguientes condiciones de regularidad:

\begin{enumerate}
    \item $\Theta$ es un intervalo de $\mathbb{R}^s$
    \item El soporte de f no depende de $\theta$. $\{x:f(x,\theta)>0\}$ no depende de $\theta$.
    \item $\frac{d}{d \theta_j} f(x,\theta)$ existe y es finita $\forall j=1,\dots,s \quad \theta \in \Theta$
    \item La matriz de información de Fisher (IF($\theta$)) es definida positiva (si $\forall X \neq A, \quad X^T \cdot A \cdot X >0$)
    \item $f(x,\theta) \,dx$ se puede derivar bajo el signo integral
\end{enumerate}

Siendo $\theta_0$ el verdadero valor del parámetro, se obtienen los siguientes resultados:

\begin{enumerate}
    \item $P_\theta(L(\theta_0,X)\geq L(\theta,X),\quad \forall \theta \in \Theta) \xrightarrow[n \to \infty]{L}1$
    
    Con probabilidad que tiende a 1, cuando la función de verosimilitud alcanza el máximo en el verdadero valor del parámetro
    \item $\hat{\theta_n} \to \theta_0$. El estimador es consistente.
    \item $\hat{\theta_n} \sim N_s(\theta_0,V(\theta))$ donde $V(\theta)=I^{-1}(\theta)$ (es la varianza de la información de Fisher esperada).
\end{enumerate}






