\section{Estimadores en muestras grandes}

Sean $X_1, X_2, \dots, X_n$ variables aleatorias independientes igualmente distribuidas (v.a.i.i.d.) tal que $P_\theta:\theta \in \Theta$. Tenemos nuestra muestra aleatoria simple (m.a.s) con $n$ grande y nos interesa estimar $\theta$ (o $g(\theta)$).

Cuando el tamaño de la muestra aumenta, también aumenta la información disponible de $\theta$ (o $g(\theta)$), por lo tanto se espera que la estimación sea más precisa. Esto podemos comprobarlo al analizar la distribución de la media muestral ya que como $\overline{X}\thicksim N\left(\mu, \frac{\sigma^2}{n}\right)$, entonces $Var(\overline{X}) \xrightarrow{n \rightarrow \infty} 0$.

\begin{figure}[h!]
    \begin{center}
        \begin{tikzpicture}
            \begin{axis}[no markers, domain=0:10, samples=100, axis lines*=left, xlabel=Test,
                    ylabel=, height=6cm, width=10cm, enlargelimits=false, clip=false, axis on top,
                    grid = major,legend style={at={(0.5,-0.1)},anchor=north, legend columns=-1}]

                \addplot [fill=cyan!20, draw=blue, domain=-4:4] {gauss(0,0.6)} \closedcycle;
                \addlegendentry{N(0,0.8)}
                \addplot [fill=orange!20, fill opacity=0.5, draw=red, domain=-4:4] {gauss(0,1)} \closedcycle;
                \addlegendentry{N(0,1)}
            \end{axis}
        \end{tikzpicture}
        \caption{Comparación aumento de n}
    \end{center}
\end{figure}

Consideraremos que un estimador $T(X_1, \dots, X_n)$ es razonable si cuando $n$ aumenta, el estimador $T(X_1, \dots, X_n)$ es más preciso. Lo que podemos esperar es que $T(X_1, \dots, X_n)$ esté próximo a $\theta$ con mayor probabilidad. \\

A continuación, se definirán los criterios que nos permitirán comprobar la calidad de información que nos proporciona un estimador.

\subsection{Consistencia de un estimador}

Se dice que un estimador $T(X_1,\dots,X_n)$ es consistente si cumple que

\[
    \forall \epsilon > 0, \delta > 0 \quad \exists N: n \geq N
\]
\[
    P_\theta(|T(X_1, \dots, X_n) - \theta| < \epsilon) \geq 1 - \delta
\]

\textbf{\textit{Definición: }} $T_n(X)$ es un estimador consistente para $\theta$ (o $g(\theta)$) si

\[
    \forall \epsilon > 0 \quad P_\theta\left(|T(X_1, \dots, X_n) - \theta| \leq \epsilon\right) \xrightarrow{n \to \infty} 1
\]

es decir, si:

\[
    T_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta \quad \text{o} \quad \lim_{n \to \infty} \forall \epsilon > 0 \quad P_\theta(|T(X_1, \dots, X_n) - \theta| \leq \epsilon) = 1
\]

Existen algunas estrategias para comprobar si un estimador es consistente:
\begin{enumerate}
    \item \textbf{Convergencia en probabilidad}. Si $T_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta$ y $G_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta'$
          \begin{itemize}
              \item $T_n(x)+G_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta + \theta '$
              \item $T_n(x)G_n(x) \overset{p}{\underset{n \to \infty}{\to}} \theta\theta '$
              \item Si $\theta' \neq 0, \frac{T_n(x)}{G_n(x)} \overset{p}{\underset{n \to \infty}{\to}} \frac{\theta}{\theta '}$
          \end{itemize}

    \item \textbf{Ley débil de los grandes números}. $X_1, X_2, \dots, X_n$ i.i.d. con $E(X_i)=\mu < \infty$
          \[\frac{1}{n}\sum_{i=1}^{n} X_i \overset{p}{\underset{n \to \infty}{\to}} \mu\]
          Los momentos muestrales son estimaciones consistentes de los correspondientes momentos poblacionales.

    \item \textbf{Convergencia en media cuadrática}. Un estimador $T(X_1, \dots, X_n)\xrightarrow{\underset{n \to \infty}{\text{m.c.}}} \theta$ si:
          \begin{itemize}
              \item $E(T(X_1, \dots, X_n)) \xrightarrow{{n \to \infty}} \theta$, sesgo de $T(X_1, \dots, X_n)\xrightarrow{{n \to \infty}} 0$
              \item $Var(T(X_1, \dots, X_n)) \xrightarrow{{n \to \infty}} 0$
          \end{itemize}
          Es decir que si ECM $T(X_1, \dots, X_n) \xrightarrow{{n \to \infty}} 0$, el estimador converge en media cuadrática. Un estimador insesgado sería consistente si su varianza converge a 0 con $n \to \infty$. La convergencia en media cuadrática es más fuerte que la convergencia en probabilidad.
\end{enumerate}

\begin{theorem}
    Sean $X_1, \dots, X_n$ v.a.i.i.d. con $E(X_i)=\mu$ y $Var(X_i)=\sigma^2$, $\forall i=1, \dots, n$. ¿Es consistente $T_n(x) = \frac{X_1 + X_2 + \dotsb + X_n}{\frac{n}{2}}$?

    Usando la ley de los grandes números:
    \[
        \frac{1}{n}\sum X_i \xrightarrow{\text{P}} \mu \quad T_n(x)\xrightarrow{P}2\mu
    \]

    Resultado: El estimador $T_n(x)$ NO es consistente
\end{theorem}

La consistencia por si sola no es tan interesante, ya que si $T_n$ es consistente para $\theta$, nos dice que para n grande los errores serán pequeños pero no nos permite conocer el orden del error $\left(\frac{1}{n}, \frac{1}{\sqrt{n}},\frac{1}{\log(n)}, \dots\right)$

Si $\{K_n\}_{n=1}^{\infty}$ es una sucesión de reales positivos y $\epsilon>0$ definimos $P_n(\epsilon)$ como

\[P_n(\epsilon)=P_\theta(|T_n(x)-\theta| \leq \frac{\epsilon}{K_n})\]

Habiendo definido $P_n(\epsilon)$, ¿qué pasará con $P_n(\epsilon)$ cuando $n$ sea grande?
\begin{enumerate}
    \item Si $K_n$ crece "lentamente" (por ejemplo:  $K_n=\log(n)$), el error disminuye "lentamente" a medida que aumenta n $P_n(\epsilon) \xrightarrow{{\text{n} \to \infty}} 1$. Si $K_n$ crece lentamente, $ \frac{\epsilon}{K_n}$ es más grande, lo que facilitará que el error esté por debajo del umbral.
    \item Si $K_n$ crece "rápido" (por ejemplo:  $K_n=n$), el error disminuye más rápido a medida que aumenta n $P_n(\epsilon) \xrightarrow{{\text{n} \to \infty}} 0$.  Si $K_n$ crece rápido, $ \frac{\epsilon}{K_n}$ se hace muy pequeño y se hace muy difícil que el error sea tan pequeño.
    \item Casos intermedios. Si $K_n$ crece "adecuadamente", $P_n(\epsilon) \xrightarrow{{\text{n} \to \infty}} H(\epsilon) \in (0,1)$. Decimos que el error converge a 0 a velocidad $\frac{1}{K_n}$
\end{enumerate}

De manera resumida:

\[
    P_n(\epsilon) = P_\theta(K_n|T_n(x) - \theta| \leq \epsilon) \xrightarrow{n \to \infty}\begin{Bmatrix}
        0 \text{ si } K_n \xrightarrow{\infty} \text{rápido}                                  \\
        0 \leq P_n(\epsilon) \leq 1 \text{ si } K_n \xrightarrow{\infty} \text{adecuadamente} \\
        1 \text{ si } K_n \xrightarrow{\infty} \text{lento}
    \end{Bmatrix}
\]

La idea es que al multiplicar $K_n$, se amplifica la velocidad de convergencia de los errores a 0. Si elegimos $K_n$ adecuadamente, de forma que $P_n(\epsilon)$ sea menor que 1, podemos controlar la velocidad a la que los errores tienden a 0, mejorando la precisión.

\subsection{Estimador Consistente Asintoticamente Normal (CAN)}

Sean $X_1,\dots,X_n$ v.a.i.i.d. con distribución $P_\theta$ donde $\theta \in \Theta$, entonces un estimador $T_n(X)=T(X_1,\dots,X_n)$ con $n\geq 1$ será CAN para $\theta$ si

\[
    \exists \sigma_n(\theta) > 0, \theta \xrightarrow{n\to\infty}0 \quad \text{tal que}\quad \frac{T_n(X)-\theta}{\sigma_n(\theta)}\overset{L}{\to}N(0,1)
\]

En muchas ocasiones, $\sigma^2_n(\theta)$ es de la forma $\frac{Var_\theta(T_n(X))}{n}$ y se llama varianza asintótica del estimador. En este caso $T_n(X)$ es CAN para $\theta$ si

\[
    \sqrt{n}(|T_n(X)-\theta|) \overset{L}{\to} N(0, Var_\theta(T_n(X)))
\]

donde los errores convergen a 0 con $\frac{1}{\sqrt{n}}$. \\

Al igual que la LDGN es clave para obtener estimadores consistentes, el TCL lo es para obtener estimadores CAN. \\
Sean $X_1,\dots,X_n$ v.a.i.i.d. tal que $E(X_i)=\mu$ y $Var(X_i)=\sigma^2 < \infty$. Entonces

\begin{itemize}
    \item $\frac{\sum{X_i} - E(\sum{X_i})}{\sqrt{Var(\sum{X_i})}} \overset{L}{\to} N(0,1)$
    \item $\sqrt{n}\frac{\overline{X}-\mu}{\sigma}\overset{L}{\to}N(0,1)$
    \item $\overline{X}\thicksim N\left(\mu,\frac{\sigma^2}{n}\right)$
\end{itemize}

Resulta útil aplicar el $\Delta$-método cuando queremos aproximar la distribución de una función no lineal de un estimador que es asintoticamente normal.\\
Sean $X_1,\dots,X_n$ v.a.i.i.d. donde su distribución es $P_\theta$ tal que $\theta \in \Theta \subseteq \mathbb{R}$ y sea uan función $g:\Theta \to \mathbb{R}$ con derivada no nula. Si $T_n(X)$ es CAN para $\theta$ tal que...
% no me da tiempo a seguir xd
