\begin{enumerate}
    \item Es importante ver que la máxima diferencia en $|\widehat{F_n}(\chi)-F_0(\chi)|$ es la máxima 
    diferencia en los puntos en los que $\widehat{F_n}(x)>F_0(x)$ y la máxima en los punto donde $F_0(x)>\widehat{F_n}(x)$. 
    Así podemos definir los estadísticos Kolmogorov-Smirnov de un lado.
    \[
    \left.
    \begin{matrix}
        D_n^+=sup_\chi(\widehat{F_n}(x)-F_0(x)) \\
        D_n^-=sup_\chi(F_0(x)-\widetilde{F_n}(x))
    \end{matrix}
    \right\}
    D_n=max\{D_n^+,D_n^-\}
    \]
    $D_n^+$ y $D_n^-$ son útiles para conocer la distribución.
    \item La distribución de $D_n^+,D_n^-$ y $D_n$ no depende de $F_0$, es decir, $c_\alpha$
    será el mismo sin importar si estamos contrastando diferentes hipótesis simples (Por ejemplo: N(3,1);exp(2),$\dots$)
    Se dice que el estadístivo es de distribución libre ya que no depende de $F_0$.
\end{enumerate}

Definimos los estadísticos de orden como $x_{(0)}=-\infty$ y $x_{(n)}=+\infty$, por lo tanto podemos definir $\widehat{F_n}(x)=\frac{i}{n} \quad \forall \chi_{(i)\leq \chi < \chi_{i+1}}$.

$D_n^+$ lo podemos ir calculando a trozos como:

\[
    D_n^+=sup_\chi(\widehat{F_n}(x)-F_0(x))
\]

Tomará un valor máximo unicamente en los extremos del intervalo $[x_{(i)},x_{(i+1)})$

%insertar imagen de los apuntes.
\[
    D_n^+=max_{1 \leq i \leq n}\left(\frac{i}{n}-F_0(x_{(i)})\right)
\]
Análogamente:
\[
    D_n^-=max_{1 \leq i \leq n}\left(F_0(x_{(i)})-\frac{i-1}{n}\right)
\]
$D_n^+,D_n^-$(por lo tanto $D_n$) dependen de las variables aleatorias.
\[
F_0(X_{(1)}),\dots,F_0(X_{(n)}) \sim U(0,1)
\]
Esto ocurre por la trasnformada integral de probabilidad, siempre y cuando las variables aleatorias sean continuas.

%Insertar imagen? Ns quien vea esto q se lo cuestione.

\subsection{Test de Kolmogorov-Smirnov para una hipótesis compuesta}

Lo anterior nos sirve para hipótesis simp,es. Sin embargo en la mayoría de los escenarios reales, tenemos hipótesis compuestas.
Por ejemplo, utlizar Kolmogorov-Smirnov en ANOVA.

Situación:
\[
H_0: F=F_0(\theta) \quad H_1:F \neq F_0(\theta)
\]
Lo haremos como siempre:
\begin{enumerate}
    \item Estimamos $\theta$ a partir de los datos(con el Estimador Máximo Verosimil).
    \item Se calcula el estadístico Kolmogorov-Smirnov como en el caso de la hipótesis simple:
    \[
    \widehat{D_n}=sup_\chi|\widehat{F_n}(\chi)-F(\chi)|
    \]
    Se rechaza $H_0$ con valores grandes de $\widehat{D_n}$ ($\widehat{D_n}>C$). 
    
    Es importante recalcar que $\widehat{D_n}$ no sigue la misma distribución que $D_n$.
    La distribución no es libre, depende de la familia que estemos evaluando.
\end{enumerate}

Tenemos tablas obtenidas por \href{https://es.wikipedia.org/wiki/Prueba_de_Lilliefors}{Lilliefors}
por simulación para la normal y para la exponencial.
Pasos a seguir:
\begin{enumerate}
    \item Se estiman los parámetros $\widehat{mu}$ y $\widehat{\lambda}$
    \item Se obtiene la muestra estandarizada $z_1,\dots,z_n$.
    \item Hacer el test ($H_0:F_0 \equiv N(0,1)$ en el caso normal y $H_0:F_0\equiv exp(1)$ en el caso exponencial).
    \item Se rechaza $H_0$ si $\widehat{D_n}>C$ para nivel $\alpha$. Se usan las tablas de \href{https://es.wikipedia.org/wiki/Prueba_de_Lilliefors}{Lilliefors}
    para definir $C_\alpha$.
\end{enumerate}


\subsubsection*{Caso exponencial:}

\[
\begin{matrix}
    H_0:F \equiv exp(\lambda) \\
    H_1:F \neq exp(\lambda)
\end{matrix}
\]
\[
\widehat{\lambda}=\frac{1}{\bar{X}}, \quad z_i
=\frac{X_i}{\bar{X}} \to z_i \sim exp(1) \longrightarrow\gamma(1,\lambda)
\]
El contraste es equivalente a:
\[
H_0: X \sim \gamma\left(1,\frac{1}{\bar{X}}\right) \longleftrightarrow H_0:Z \sim \gamma(1,1)
\]

Hay más estadísticos en el campus

\section{Test de Shapiro-Wilk}
(Este test es más potente que el de Kolmogorov-Smirnov)

Mide como de bien se ajustan los datos a una distribución normal esperada. 
Se basa en combinación lineal de estadísticos ordenados. %Esto lo ha añadido en clase

\[
W=\frac{\left(\sum_{i=1}^{n} a_i \cdot \chi_{(i)}\right)^2}{\sum_{i=1}^{n}(\chi_i-\bar{X})^2}
\]

donde:
\begin{itemize}
    \item $\chi_{(i)} \longrightarrow$ estadístico de orden i.
    \item $a_i \longrightarrow$ coeficientes calculados a partir de los cuartiles esperados de una distribución normal estandar. 
\end{itemize}

\[
(a_1,a_2,\dots,a_n)=\frac{m^T\cdot v^{-1}}{\sqrt{m^T \cdot V^{-1} \cdot V^{-1} \cdot m}}
\]
\newpage
donde:
\begin{itemize}
    \item m=($m_{(1)},\dots,m_{(n)}$)
    \item V es la matriz de las covarianzas de $m_{(i)}$
\end{itemize}

El numerador nos indica como de bien se alinean los datos con la normalidad esperada. Si los datos son normales, el numerador será grande.








