\section{Introducción al Bootstrap}

El bootstrap es un mecanismo generador de datos. Hasta ahora hemos trabajado en una situación en la que tenemos una muestra $X=(X_1,\dots,X_n)$ v.a.i.i.d. de una distribución $P_\theta$, $\theta=(\theta_1,\dots,\theta_s)$ con el interés de obtener un estimador $T(\theta)$ razonable para $\theta$ o $g(\theta)$.\\

Todo ello en el concepto de \textbf{inferencia frecuentista}; se tiene un estimador del parámetro en base al que queremos hacer inferencia respecto a $\theta$.
Para esto es necesario conocer la distribución del estadístico  (distribución exacta o asintótica). Supongamos que:
\begin{itemize}
    \item No conocemos la distribución de los datos
    \item No se cumplen las condiciones de regularidad de Cramer-Rao
\end{itemize}

Cuando se da uno de los casos anteriores, el bootstrap puede ser una buena opción. Puede resultar interesante poder repetir un mecanismo generador de datos con el que se obtuvo la muestra original de forma que podamos obtener tales muestras como se quiera, y cada una de ellas obtendrá sus estadísticos correspondientes. Es decir, a partir de $P_\theta$ obtendremos:

$$\left.
    \begin{matrix}
        X_{11}     & X_{12}     & \cdots & X_{1n}     & \longrightarrow & T^{1}_n(X)    \\
        X_{21}     & X_{22}     & \cdots & X_{2n}     & \longrightarrow & T^{2}_n(X)    \\
        \vdots     & \vdots     & \ddots & \vdots     & \vdots          & \vdots        \\
        X_{5000,1} & X_{5000,2} & \cdots & X_{5000,n} & \longrightarrow & T^{5000}_n(X)
    \end{matrix}
    \right\}\text{Simulaciones si $P_\theta$ es conocida}$$

Podemos usar bootstrap para \textbf{aproximar cualquier característica} de la distribución y hacer inferencia a partir de los datos simulados.\\

Sin embargo, no siempre se conoce $P_\theta$, si no que solo se dispone de los datos observados. En estos casos no es posible simpular a partir de $P_\theta$.
Podremos simularlos si somos capaces de estimar $F_\theta(\cdot)$, la verdadera función de distribución, La estimaremos a partir de la distribución empírica:

$$\hat{F}(X)=\frac{1}{n}\sum_{i=1}^{n}\text{\Large{$\mathbbm{1}_{(x_i\leq x)}$}}$$

Donde $\mathbbm{1}$ se refiere a la función indicadora. \textit{En los apuntes del tema 3 de probabilidad (2º curso) se ve otra forma de definir la función de distribución empírica}.\\

\textbf{\textit{Definición:} Principio plug-in:} cualquier característica de una distribución pueede ser aproximada. El principio plug-in está apoyado por el \textbf{Teorema de Glivenko-Cantelli}:
$$\underset{x\in\mathbb{R}}{sup}\ |\hat F(X)-F_0(X)|\overset{c.s.}{\longrightarrow}0$$

La idea es simular por remuestreo el experimento y, a continuación, reajustar el modelo y recalcular estimadores con los datos simulados. Estos serían los pasos:
\begin{enumerate}
    \item Estimar $F_0(X)$ a partir de la muestra
    \item Simular $\hat F(X)$
\end{enumerate}
Con el bootstrap podemos obtener también estimadores sobre el sesgo, intervalos de confianza y contrastes de hipótesis.

\subsection{Aproximación bootstrap de la distribución EMV}

Sean $X_1,\dots,X_n$ con $F(\dot)$, $\hat\theta$ es el EMV de $\theta$. El bootstrapsimula la distribución de $\hat\theta$.
\begin{enumerate}
    \item Se estima $\hat F(X)$$\begin{cases}
              \text{En el caso no paramétrico, a partir de la función de distribución empírica} \\
              \text{En el caso paramétrico, estimando los parámetros necesarios}
          \end{cases}$
    \item Generamos datos artificiales: las muestras bootstrap:
          \begin{itemize}
              \item $X_1^*,\dots,X_n^*$ con función de densidad $\hat F$ estimada de $F$
              \item Se obtiene el EMV $\hat\theta^*$ basado en la muestra bootstrap
          \end{itemize}
\end{enumerate}

La idea del procedimiento es la siguiente: la distribución $\hat\theta^*-\hat\theta$ aproxima la distribución de $\hat\theta-\theta$. Al repetir los pasos anteriores en un proceso B veces se obtiene una versión bootstrap del EMV.

Existen dos tipos de bootstrap:
\begin{itemize}
    \item \textbf{Bootstrap paramétrico:} si el estimador de $F$ en el paso 1 es un estimador paramétrico
    \item \textbf{Bootstrap no paramétrico:} si usamos la función de distribución empírica para estimar $F$ en el paso 1
\end{itemize}

\hspace{-1cm}\noindent\begin{tabular}{r}
    \textbf{Ejemplo} \\ \hline \ \\
\end{tabular}

Sean $X_1,\dots,X_n$ v.a.i.i.d de una $N(\mu,\sigma^2)$. Según lo visto en temas anteriores, sabemos que el EMV para $\theta=(\mu,\sigma)$ sigue la siguiente distribución:
$$\sqrt(n)\begin{pmatrix}
        \hat\mu-\mu \\
        \hat\sigma-\sigma
    \end{pmatrix} \overset{\mathcal{L}}{\longrightarrow}N_2\Bigg(\begin{pmatrix}
            0 \\ 0 \end{pmatrix},\begin{pmatrix}
            \sigma^2 & 0 \\ 0 & 4\sigma^2
        \end{pmatrix}\Bigg)$$
En este ejercicio supondremos que sabemos que los datos vienen de una distribución normal (aunque desconocemos los valores de $\mu$ y $\sigma$). Para conseguir una muestra bootstrap tenemos que seguir los pasos ya mencionados anteriormente:
\begin{enumerate}
    \item Estimar $\mu$ y $\sigma$ a partir del EMV
    \item Simular desde nuestra nueva $\hat F$ (que sabemos que sigue una distribución normal)
\end{enumerate}
(\textit{El algoritmo se ejecuta de forma iterativa, y es un proceso muy laborioso a mano. Por ello, se deja como ejercicio propuesto al lector hacer un script que siga dichos pasos (se ve en clases prácticas.})

\noindent La distribución de $\begin{pmatrix}\mu^*_i \\ \sigma^{2*}_i\end{pmatrix}-\begin{pmatrix}\hat\mu \\ \hat\sigma^{2}\end{pmatrix}\forall i=1,2,\dots,B$ aproxima la distribución de $\begin{pmatrix}\hat\mu \\ \hat\sigma^{2}\end{pmatrix}-\begin{pmatrix}\mu \\ \sigma^{2}\end{pmatrix}$\\
De igual forma, el histograma de $\mu^*_1,\dots,\mu^*_B$ aproxima el de $\hat\mu$. Pasa lo mismo para $\sigma^2$

En el caso no paramétrico estimaríamos F a partir de la muestra original y su función de distribución empírica.

\subsubsection{Estimador bootstrap de la varianza del EMV}

Estamos estudiando la distribución de $\theta$, por lo que tenemos que poder estudiar cualquier característica que dependa de $\theta$. Estimaremos la varianza del EMV usando las B muestras bootstrap.

\textit{Justificación} $\Longrightarrow$ La distribución de $\hat\theta^*-\hat\theta$ aproxima la de $\hat\theta-\theta$; por lo que la distribución de $Var(\hat\theta^*-\hat\theta)$ aproxima la de $Var(\hat\theta-\theta)$, y, por tanto 
$Var^*(\hat\theta^*)\approx Var^*(\hat\theta)$

\subsection{Intervalos de confianza bootstrap (Método percentil)}
Consideremos $X_1,\dots,X_n$, con función de distribución $F_0(\dot)$, dependiente del parámetro s-dimensional $\theta=(\theta_1,.s,\theta_s)$. Para obtener un intervalo de confianza de nivel 1-$\alpha$ para la k-ésima componente de $\theta$ ($\theta_k$) utilizaremos el \textbf{método percentil}.\\

Sean $\hat\theta_{k1}^*,\dots,\hat\theta_{kB}^*$ las B versiones bootstrap del estimador $\hat\theta_k$, y sean $\hat\theta^*_{k,\frac{\alpha}{2}}$ y $\hat\theta^*_{k,1-\frac{\alpha}{2}}$ los cuantiles $\alpha/2$ y $1-\alpha/2$ respectivamente.
El intervalo de confianza bootstrap percentil para $\theta_k$ será $\left(\hat\theta^*_{k,\frac{\alpha}{2}},\hat\theta^*_{k,1-\frac{\alpha}{2}} \right)$. Es decir, el método percentil consiste en sustituir los extremos del intervalo por los percentiles correspondientes para nuestro nivel $\alpha$.

$$P_\theta\left(\hat\theta^*_{k,\frac{\alpha}{2}}\leq\theta\leq\hat\theta^*_{k,1-\frac{\alpha}{2}}\right)\approx 1-\alpha$$

La justificación viene dada por la suposición de que, bajo las condiciones de regularidad apropiadas, el comportamiento de $\hat\theta$ como estimador de $\theta$ sea parecido al comportamiento de $\hat\theta^*$ como estimador de $\hat\theta$.
En otras palabras, un intervalo que contenga a $\hat\theta^*_{k}$ con probabilidad aproximada $1-\alpha$ es también un intervalo que contiene a $\theta_k$ con probabilidad aproximada $1-\alpha$.

En cuanto a una transformación $g(.)$, si la función g es monótona creciente, el método percentil es \textbf{invariante a transformaciones}.

\subsection{Contrastes de hipótesis bootstrap}

Sean $X_1,\dots,X_n$ i.i.d. con $f(.,\theta), \theta\in\Theta$. Vamos a contrastar $H_0: \theta\in\Theta_0$ ; $H_1:\theta\notin\Theta_0$.

Sea T el estadístico de contraste y $\left\{T \geq C_\alpha\right\}$ la región crítica de nivel $\alpha$. Existen situaciones en las que es posible calcular la distribución asintótica o exacta del estadístico T bajo la hipótesis nula.
En ese caso, podemos determinar directamente $C_\alpha$ y calcular el p-valor del test. En el caso en el que calcular la distribución de T no sea posible podemos aproximarla mediante bootstrap.

\begin{itemize}
    \item \textbf{$H_0$ es simple}: el bootstrap no es necesario, basta con simulación. La idea es simular un número grande de muestras (B) de tamaño n de la distribución; sean $T_1,\dots,T_B$ los valores del estadístico T observados en las B muestras, podemos aproximar $C_\alpha$ por el cuantil $1-\alpha$ de estos B valores y el p-valor del test por la proporción de estos valores mayores que el observado para los datos originales.
    \item \textbf{$H_0$ es compuesta}: bootstrap paramétrico. Si $\hat\theta_0$ es el EMV de $\theta$ bajo $H_0$, se generan B muestras \textbf{bootstrap} de la distribución. Al igual que antes se calcula el p-valor del test a partir de la proporción de los valores mayores que el valor observado para los datos; pero esta vez con muestreo bootstrap.
\end{itemize}
